{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d227889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from data_loaders.dataloader3d import get_dataloader, load_data, MotionDataset, TestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb9645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft-DTW loss: tensor([46646.1094], grad_fn=<SoftDTWcpuBackward>)\n"
     ]
    }
   ],
   "source": [
    "import pysdtw\n",
    "sdtw = pysdtw.SoftDTW(gamma=1.0, use_cuda=True)\n",
    "loss = sdtw(a, b)\n",
    "print(\"Soft-DTW loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "814a927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_root(data):\n",
    "    #only after frames have been cut\n",
    "    root = (data[0,8,:]+data[0, 9, :])/2\n",
    "    data=np.delete((data - root), (1,8), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6a58052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_frames(data):\n",
    "    \"\"\"\n",
    "    Drop frames where all 25 rows are identical.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        Array with shape (frames, 25, 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Filtered array with duplicate frames removed\n",
    "    \"\"\"\n",
    "    first_row = data[:, 0:1, :]  # Shape: (frames, 1, 5)\n",
    "    all_rows_same = np.all(data == first_row, axis=(1,2))\n",
    "\n",
    "    mask = ~all_rows_same\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a98f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = load_data(\"mydataset\", split='train', keypointtype='6d')\n",
    "#a,b,c = load_data(\"test_dataset\", split='test', keypointtype='openpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda9acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7e5c6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # shapes of the first sample\n",
    "test=a[0].unsqueeze(0)\n",
    "test2=test[:,-1:,:]\n",
    "test2=test2\n",
    "test2[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9570dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape[1]==69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44cf036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TestDataset(\n",
    "    \"gait\",\n",
    "    a,\n",
    "    b,\n",
    "    betas=c,\n",
    "    input_motion_length=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c4570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MotionDataset(\n",
    "    \"gait\",\n",
    "    a,\n",
    "    b,\n",
    "    input_motion_length=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b422f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(\n",
    "    dataset, \"train\", batch_size=1, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de0a7821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "<class 'data_loaders.dataloader3d.TestDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))        # number of samples\n",
    "print(type(dataset))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a83cf5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 135]) torch.Size([90, 135]) 90\n"
     ]
    }
   ],
   "source": [
    "img, label, betas = dataset[5]\n",
    "print(betas.shape, label.shape, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import pose_distance_metric\n",
    "import fastdtw\n",
    "test, _=fastdtw.fastdtw(betas[:,3:], label[:,3:], dist=pose_distance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaee1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0803,  0.7028, -0.2629,  ...,  0.1575, -0.8400,  0.1474],\n",
      "        [-0.1069,  0.6896, -0.2982,  ...,  0.1442, -0.8400,  0.1305],\n",
      "        [-0.1376,  0.6814, -0.3331,  ...,  0.1469, -0.8439,  0.1288],\n",
      "        ...,\n",
      "        [-0.5876,  0.7092, -1.2383,  ..., -0.5444, -0.8682, -1.1108],\n",
      "        [-0.5974,  0.7130, -1.2742,  ..., -0.5391, -0.8297, -1.0494],\n",
      "        [-0.6122,  0.7205, -1.3082,  ..., -0.5630, -0.8578, -1.1286]])\n"
     ]
    }
   ],
   "source": [
    "print(label)  # number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9905fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 165, 25, 5]) torch.Size([1, 240, 69]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    images, labels,betas = batch\n",
    "    print(images.shape, labels.shape, betas.shape)  # shapes of the batch\n",
    "    break  # Just process the first batch for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bfde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([91, 135])\n",
      "torch.Size([75, 135])\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "print(a[i].shape)\n",
    "print(b[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e223637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (5, 2, 2)\n",
      "Cleaned:  (3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_padding_3d_numpy(sequence):\n",
    "    # sequence shape: (frames, x, y)\n",
    "    \n",
    "    # 1. Compute difference between frames\n",
    "    # Shape: (frames-1, x, y)\n",
    "    diffs = sequence[1:] - sequence[:-1]\n",
    "    \n",
    "    # 2. Check if ANY value changed in the (x, y) frame\n",
    "    # We look across axis 1 and 2. \n",
    "    # If the frame is 0 everywhere, it returns False.\n",
    "    non_zero_diff = np.any(diffs != 0, axis=(1, 2))\n",
    "    \n",
    "    # 3. Get indices\n",
    "    change_indices = np.where(non_zero_diff)[0]\n",
    "    \n",
    "    if len(change_indices) == 0:\n",
    "        return sequence[:1]\n",
    "    \n",
    "    last_real_index = change_indices[-1] + 2\n",
    "    \n",
    "    return sequence[:last_real_index]\n",
    "\n",
    "# --- Test Case ---\n",
    "# 5 Frames, 2x2 Image size\n",
    "data = np.zeros((5, 2, 2))\n",
    "data[0] = [[1, 1], [1, 1]]\n",
    "data[1] = [[2, 2], [2, 2]]\n",
    "data[2] = [[3, 3], [3, 3]] # Last real frame\n",
    "data[3] = [[3, 3], [3, 3]] # Padding (Duplicate)\n",
    "data[4] = [[3, 3], [3, 3]] # Padding (Duplicate)\n",
    "\n",
    "cleaned = remove_padding_3d_numpy(data)\n",
    "print(f\"Original: {data.shape}\") # (5, 2, 2)\n",
    "print(f\"Cleaned:  {cleaned.shape}\")  # (3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50150eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load: mydataset\\gait_682\\20250919_c2_a3_Take2\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Could not load: mydataset\\gait_682\\20250919_c2_a3_Take3\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Could not load: mydataset\\gait_682\\20250919_c2_a4_Take1\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Could not load: mydataset\\gait_682\\20250919_c2_a4_Take2\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Could not load: mydataset\\gait_682\\20250919_c2_a5_Take1\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Could not load: mydataset\\gait_682\\20250919_c2_a5_Take2\\split_subjects\\0\\keypoints_3d\\smpl-keypoints-3d_cut.npy\n",
      "Overall min and max values::\n",
      "-1.6090182703484142 0.38673450951507693\n",
      "-0.9003782868966556 0.8199114286130842\n",
      "-2.930964477812297 0.5061700658363381\n",
      "Shapes: 238 31\n"
     ]
    }
   ],
   "source": [
    "minx=5000\n",
    "maxx=-1000\n",
    "miny=5000\n",
    "maxy=-1000\n",
    "minz=5000\n",
    "maxz=-1000\n",
    "root=\"mydataset\"\n",
    "shapes=[]\n",
    "\n",
    "for patient in os.listdir(root):\n",
    "    for file in sorted(os.listdir(os.path.join(root, patient))):\n",
    "\n",
    "        mypath=os.path.join(root, patient, file)\n",
    "        mypath=os.path.join(mypath,\"split_subjects\", \"0\", \"keypoints_3d\", \"smpl-keypoints-3d_cut.npy\")\n",
    "        #print(mypath)\n",
    "        \n",
    "        try:\n",
    "            data=np.load(mypath)\n",
    "        except:\n",
    "            print(\"Could not load:\", mypath)    \n",
    "            continue\n",
    "        data=drop_duplicate_frames(data)\n",
    "        shapes.append(data.shape[0])\n",
    "        #print(data.shape)\n",
    "        data=subtract_root(data)\n",
    "        datax=data[...,0]\n",
    "        datay=data[...,1]\n",
    "        dataz=data[...,2]\n",
    "\n",
    "        curminx=np.min(datax)\n",
    "        curmaxx=np.max(datax)\n",
    "        curminy=np.min(datay)\n",
    "        curmaxy=np.max(datay)\n",
    "        curminz=np.min(dataz)\n",
    "        curmaxz=np.max(dataz)\n",
    "\n",
    "        if curminx < minx:\n",
    "            minx = curminx\n",
    "        if curmaxx > maxx:\n",
    "            maxx = curmaxx\n",
    "        if curminy < miny:\n",
    "            miny = curminy\n",
    "        if curmaxy > maxy:\n",
    "            maxy = curmaxy\n",
    "        if curminz < minz:\n",
    "            minz = curminz\n",
    "        if curmaxz > maxz:\n",
    "            maxz = curmaxz\n",
    "print(\"Overall min and max values::\")\n",
    "print(minx, maxx)\n",
    "print(miny, maxy)\n",
    "print(minz, maxz)\n",
    "print(\"Shapes:\", max(shapes), min(shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cfda8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 25, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmydataset/gait_753/20250617_c1_a1_Take1/split_subjects/0/fit-smplx/smpl-keypoints-3d_cut.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeys inside:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mfiles:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k, data[k]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'files'"
     ]
    }
   ],
   "source": [
    "data = np.load('mydataset/gait_753/20250617_c1_a2_Take1/split_subjects/0/keypoints_3d/smpl-keypoints-3d.npy')\n",
    "print(data.shape)\n",
    "data= np.load('mydataset/gait_753/20250617_c1_a1_Take1/split_subjects/0/fit-smplx/smpl-keypoints-3d_cut.npy')\n",
    "\n",
    "print(\"Keys inside:\", data.files)\n",
    "for k in data.files:\n",
    "    print(k, data[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad979f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys inside: ['betas', 'global_orient', 'body_pose', 'transl', 'left_hand_pose', 'right_hand_pose', 'jaw_pose', 'leye_pose', 'reye_pose', 'expression']\n",
      "betas (121, 10)\n",
      "global_orient (121, 3)\n",
      "body_pose (121, 63)\n",
      "transl (121, 3)\n",
      "left_hand_pose (121, 12)\n",
      "right_hand_pose (121, 12)\n",
      "jaw_pose (121, 3)\n",
      "leye_pose (121, 3)\n",
      "reye_pose (121, 3)\n",
      "expression (121, 10)\n"
     ]
    }
   ],
   "source": [
    "data= np.load('mydataset/gait_700/20250820_c1_a1_Take1/split_subjects/0/fit-smplx/smplx-params.npz')\n",
    "\n",
    "print(\"Keys inside:\", data.files)\n",
    "for k in data.files:\n",
    "    print(k, data[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0b484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 69)\n"
     ]
    }
   ],
   "source": [
    "data= np.load('results/generated_motion_0.npy')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da36cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transformation_sixd import smplx_to_6d, sixd_to_smplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de05fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=smplx_to_6d(\"mydataset/gait_011/20251107_c1_a1_Take1/split_subjects/0/fit-smplx/smplx-params_cut.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10466917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['motion_6d', 'transl', 'betas'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6185b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bla\n",
      "Performing forward kinematics to calculate joint positions...\n",
      "Using kid template for SMPL-X model forward kinematics.\n",
      "Extracting joints for 63 frames...\n"
     ]
    }
   ],
   "source": [
    "y=sixd_to_smplx(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb10df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695fb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_keypoints_only_mask(keypoints_path, height_thresh=0.03, vel_thresh=0.05, video_fps=30):\n",
    "    \"\"\"\n",
    "    Determines foot contact purely from kinematics (Keypoints), ignoring the Force Plate.\n",
    "    \n",
    "    Criteria for Contact:\n",
    "    1. Height: Joint Y-coordinate is close to the ground floor.\n",
    "    2. Velocity: Joint is not moving significantly (Stance phase is static).\n",
    "\n",
    "    Args:\n",
    "        keypoints_path (str): Path to .npy keypoints (frames, 25, 3).\n",
    "        height_thresh (float): Max distance from floor (e.g., 0.03m).\n",
    "        vel_thresh (float): Max velocity to be considered \"static\" (e.g., 0.05 m/frame).\n",
    "        video_fps (int): Video frame rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Binary mask of shape (frames, 25).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        keypoints = np.load(keypoints_path) # Shape (F, 25, 3)\n",
    "        n_frames = keypoints.shape[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Calculate the Floor (Ground Truth)\n",
    "    # We assume the lowest 5% of ALL foot points in the entire file represent the floor.\n",
    "    # This is robust to outliers.\n",
    "    foot_indices = [19, 20, 21, 22, 23, 24] # L/R BigToe, SmallToe, Heel\n",
    "    all_foot_y = keypoints[:, foot_indices, 1]\n",
    "    \n",
    "    # Filter out zeros (undetected points)\n",
    "    valid_y = all_foot_y[all_foot_y != 0]\n",
    "    if len(valid_y) == 0:\n",
    "        print(\"No valid foot keypoints found.\")\n",
    "        return np.zeros((n_frames, 25), dtype=int)\n",
    "        \n",
    "    floor_level = np.percentile(valid_y, 5) # 5th percentile is the estimated floor\n",
    "    print(f\"Auto-detected Floor Level: {floor_level:.4f}\")\n",
    "\n",
    "    velocity = np.linalg.norm(np.diff(keypoints, axis=0, prepend=keypoints[0:1]), axis=2)\n",
    "    \n",
    "    is_low = keypoints[:, :, 1] < (floor_level + height_thresh)\n",
    "\n",
    "    is_static = velocity < vel_thresh\n",
    "    \n",
    "    is_detected = keypoints[:, :, 1] != 0\n",
    "\n",
    "    contact_mask = is_low & is_static & is_detected\n",
    "\n",
    "    final_mask = np.zeros_like(contact_mask, dtype=int)\n",
    "    final_mask[:, foot_indices] = contact_mask[:, foot_indices].astype(int)\n",
    "\n",
    "    return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4686e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected Floor Level: 0.3254\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "path=\"mydataset/740/20251002_c1_a3_Take2/split_subjects/0/keypoints_3d/smpl-keypoints-3d.npy\"\n",
    "mask = generate_keypoints_only_mask(path, height_thresh=0.03, vel_thresh=0.04)\n",
    "data=np.load(path)\n",
    "masked_data = data * mask[:, :, np.newaxis]\n",
    "np.save('masked_keypoints.npy', masked_data)\n",
    "print('saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"mydataset/gait_740/20251002_c1_a3_Take2/split_subjects/0/keypoints_3d/smpl-keypoints-3d.npy\"\n",
    "data=np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.delete(data, (1,8), axis=1)  # remove the second joint (index 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05c650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 23, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e8f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_transformation_matrix(v):\n",
    "    \"\"\"\n",
    "    Creates a matrix that rotates v to the X-axis, mirrors Z, \n",
    "    and rotates back.\n",
    "    \"\"\"\n",
    "    # 1. Define input and target\n",
    "    v = np.array(v, dtype=float)\n",
    "    target = np.array([1, 0, 0], dtype=float)\n",
    "    \n",
    "    # Normalize input\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    if norm_v == 0: return np.eye(3)\n",
    "    a = v / norm_v\n",
    "    b = target\n",
    "\n",
    "    # 2. Compute Rotation (Rodrigues' Formula)\n",
    "    # Axis of rotation (k)\n",
    "    k = np.cross(a, b)\n",
    "    s = np.linalg.norm(k) # sin of angle\n",
    "    c = np.dot(a, b)      # cos of angle\n",
    "\n",
    "    if s == 0:\n",
    "        # Vector is already on x-axis (parallel)\n",
    "        if c > 0: return np.diag([1, 1, -1]) # Just mirror\n",
    "        # If anti-parallel (-x), we usually just flip indices, \n",
    "        # but technically needs 180 rotation. Simplified here:\n",
    "        return np.diag([1, 1, -1]) \n",
    "\n",
    "    # Skew-symmetric matrix K\n",
    "    K = np.array([\n",
    "        [0, -k[2], k[1]],\n",
    "        [k[2], 0, -k[0]],\n",
    "        [-k[1], k[0], 0]\n",
    "    ])\n",
    "\n",
    "    # Rotation Matrix (aligns v -> x)\n",
    "    # Note: We want R that takes v to x. \n",
    "    # The formula R = I + K + ... rotates a to b.\n",
    "    R = np.eye(3) + K + (K @ K) * ((1 - c) / (s**2))\n",
    "\n",
    "    # 3. Mirror Matrix (Mirror across XY plane means Z -> -Z)\n",
    "    M = np.diag([1, 1, -1])\n",
    "\n",
    "    # 4. Combine: Un-rotate * Mirror * Rotate\n",
    "    # Note: If R takes v->x, then R.T takes x->v.\n",
    "    # Order depends on if we view R as transforming the basis or the vector.\n",
    "    # Standard: T_final = R.T @ M @ R\n",
    "    return R.T @ M @ R\n",
    "\n",
    "# --- Usage ---\n",
    "input_vector = [1, 1, 1] # The vector defining the orientation\n",
    "points_to_transform = [\n",
    "    [2, 2, 2],\n",
    "    [0, 5, 0],\n",
    "    [1, 1, 1]\n",
    "]\n",
    "\n",
    "transform_matrix = get_transformation_matrix(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49b4ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(\"mydataset/gait_766\\\\20251001_c2_a3_Take3\\\\split_subjects\\\\0\\\\keypoints_3d\\\\smpl-keypoints-3d_cut.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7dabc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=data[0,2,:]+data[0, 5, :]/2\n",
    "dest=data[44,2,:]+data[44, 5, :]/2\n",
    "vec=dest - root\n",
    "vec=vec[:3]\n",
    "transform_matrix = get_transformation_matrix(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b66dea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = data[..., :3] @ transform_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cb9cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[..., :3] = transformed_data\n",
    "np.save(\"mydataset/gait_766\\\\20251001_c2_a3_Take3\\\\split_subjects\\\\0\\\\keypoints_3d\\\\smpl-keypoints-3d_cut.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb7c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sample_id': 0, 'dtw_distance': 172.32413447828506, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 187.42159815581, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 234.44739568105066, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 545.4755389302669, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 332.158031307944, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 200.84824775233537, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 374.96440842874046, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 301.52375697000866, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 517.766573394214, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 103.36017961482756, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 191.40730794960888, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 70.90764631736889, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 55.835481497143974, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 286.83543418291583, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 205.04831229220557, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 278.43688504056854, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 230.67735944318198, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 494.11519009136856, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 30.618902068160743, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 317.44703263539475, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 42.85422313855163, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 49.866321935788285, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 209.79588657840364, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 188.56048285646312, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 259.51452974773485, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 250.1677349409184, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 452.49074825772306, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 40.33404489930174, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 209.622613174691, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 35.38079257611008, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 138.67872855850368, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 157.04423178326553, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 167.25403484967595, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 174.1272873744429, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 220.11931145173622, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 717.1158675079516, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 21.58279790594626, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 127.67970973673933, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 42.81389990329609, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 33.666740458835285, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 254.90283913438802, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 159.66121577251377, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 184.64067707277067, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 273.7261789895093, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 517.1652444637124, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 37.03641150157007, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 225.5820281874721, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 36.5986378285834, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 43.38709658350202, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 251.41226156241353, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 161.2497379469188, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 232.89342413803075, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 244.92121820639457, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 609.3663057634732, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 39.13732761522822, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 96.3997267303295, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 54.84554780186093, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 46.56954767692584, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 275.3842733675866, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 144.33536619245297, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 355.7804691137071, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 197.43546916724392, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 511.2505393136068, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 74.15492374313169, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 134.51087524411278, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 56.91636137829737, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 85.03274825909301, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 251.13318381218852, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 160.3078496816939, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 211.9710139730218, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 180.62339730073137, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 480.91403994898116, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 50.875570601792816, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 140.15048866305204, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 44.61575771361362, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 61.22666502654541, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 217.95474155594775, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 111.39281477273609, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 335.21950612745877, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 205.49082495584372, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 452.96162513060165, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 146.32162263701633, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 198.66857412513318, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 147.43320354435846, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 162.80646652168002, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 266.57026006141285, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 319.9913836811999, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 313.65483332442176, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 323.49299471494294, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 467.1855576732524, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 167.41670933292835, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 187.38152595669612, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 228.81936946818152, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 178.43879171292315, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 333.98773367383814, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 269.3380620286388, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 417.80882841995015, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 419.0676156022847, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 645.9035014147377, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 167.75209492570627, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 185.54292648369216, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 155.07801897758984, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 151.12074407936439, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 258.07599737962033, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 316.8191922712725, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 298.4609388214427, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 316.92417444513814, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 489.0446092945424, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 153.88382865059657, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 157.70083444376112, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 221.34737553999756, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 203.19868943144135, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 325.39265671357236, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 340.8775846892364, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 419.10134090303916, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 431.71362747440867, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 655.1306652055027, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 20.232309370699674, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 144.04477245636755, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 27.89675995191421, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 40.75828093237994, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 265.7832678388984, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 152.67353441315208, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 181.02533519438845, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 275.1459353843767, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 506.1053226858337, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 34.57093607719309, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 168.735575949916, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 12.98560201464511, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 32.09361738543788, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 212.44106261755823, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 165.707822316606, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 318.39250222473333, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 297.61422895606785, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 604.8974835800859, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 146.55785670209858, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 209.93516796396202, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 148.88870173279633, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 146.45475610295807, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 268.61922083154764, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 331.6062860168789, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 281.98496699647706, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 339.1586731580153, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 475.04712249540256, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 22.70958031386364, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 143.6575120341175, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 20.4378506150668, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 21.02067528691951, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 221.60573335694036, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 156.1997935818373, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 195.0485143851286, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 257.2411691519618, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 585.6864416900643, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 169.20852329696672, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 174.57901527144958, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 175.98400084328148, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 185.49974542000996, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 303.41731046347286, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 257.2837180108307, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 344.9603972149307, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 352.4606747322957, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 581.3508205935218, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 323.2405786749731, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 265.26964980610404, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 300.38089259875255, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 346.57998192569477, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 569.3153531713881, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 458.1203675570998, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 765.7788106710252, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 732.4699444574298, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 496.50335788438275, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 340.82423397489725, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 485.6123399716876, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 370.8482802541917, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 420.0498253094455, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 299.6252617054457, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 267.5946166321151, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 324.7464199167389, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 338.484798343121, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 572.1213885325188, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 356.11913647218, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 515.6191661850353, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 290.3736583005785, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 573.2982925686977, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 438.2159556651496, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 297.8181636676976, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 472.44066700287834, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 492.835166560145, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 640.2939669011436, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 37.03641150157007, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 225.5820281874721, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 36.5986378285834, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 43.38709658350202, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 251.41226156241353, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 161.2497379469188, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 232.89342413803075, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 244.92121820639457, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 609.3663057634732, 'reference_frames': 268, 'generated_frames': 240}]\n",
      "[{'sample_id': 0, 'dtw_distance': 27.738340834882973, 'reference_frames': 132, 'generated_frames': 240}\n",
      " {'sample_id': 1, 'dtw_distance': 185.9099727022869, 'reference_frames': 165, 'generated_frames': 240}\n",
      " {'sample_id': 2, 'dtw_distance': 27.982477672948896, 'reference_frames': 59, 'generated_frames': 240}\n",
      " {'sample_id': 3, 'dtw_distance': 36.32170035395039, 'reference_frames': 232, 'generated_frames': 240}\n",
      " {'sample_id': 4, 'dtw_distance': 215.0448645284073, 'reference_frames': 82, 'generated_frames': 240}\n",
      " {'sample_id': 5, 'dtw_distance': 148.0321945822773, 'reference_frames': 69, 'generated_frames': 240}\n",
      " {'sample_id': 6, 'dtw_distance': 285.25386078287187, 'reference_frames': 63, 'generated_frames': 240}\n",
      " {'sample_id': 7, 'dtw_distance': 277.886534970178, 'reference_frames': 119, 'generated_frames': 240}\n",
      " {'sample_id': 8, 'dtw_distance': 558.7506270530761, 'reference_frames': 268, 'generated_frames': 240}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "for config in os.listdir(\"results/new\"):\n",
    "    if not config.startswith(\"config\"):\n",
    "        continue\n",
    "    config_path=os.path.join(\"results/new\", config)\n",
    "    for model in os.listdir(config_path):\n",
    "        data=np.load(os.path.join(config_path, model, \"dtw_metrics.npy\"), allow_pickle=True)\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10075223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def rotate_smplx_params_y_180(smplx_path: str):\n",
    "    \"\"\"\n",
    "    Loads an SMPL-X parameters file, rotates the global orientation and translation\n",
    "    180 degrees around the Y-axis, and saves the result back to the file.\n",
    "\n",
    "    Args:\n",
    "        smplx_path (str): Path to the .npz or .npy file containing the dictionary \n",
    "                          of SMPL-X parameters.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(smplx_path):\n",
    "        print(f\"Error: File not found at '{smplx_path}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading SMPL-X params from: {smplx_path}\")\n",
    "    \n",
    "    # Load the data\n",
    "    # allow_pickle is needed if the .npy contains a dictionary object\n",
    "    data = np.load(smplx_path, allow_pickle=True)\n",
    "    \n",
    "    # Convert to a mutable dictionary based on file type\n",
    "    if isinstance(data, np.lib.npyio.NpzFile):\n",
    "        # If it's a .npz (zipped), convert to dict\n",
    "        params = dict(data)\n",
    "    elif data.ndim == 0 and data.dtype == 'O':\n",
    "        # If it's a 0-d array wrapping a dict (common in some datasets)\n",
    "        params = data.item()\n",
    "    else:\n",
    "        # If it's already a dict or structured array\n",
    "        params = dict(data)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Rotate Translation (transl)\n",
    "    # ---------------------------------------------------------\n",
    "    if 'transl' in params:\n",
    "        # Transformation: (x, y, z) -> (-x, y, -z)\n",
    "        # We multiply x and z by -1\n",
    "        transl = params['transl'].copy()\n",
    "        transl[:, 0] *= -1  # Invert X\n",
    "        transl[:, 2] *= -1  # Invert Z\n",
    "        params['transl'] = transl\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Rotate Global Orientation (global_orient)\n",
    "    # ---------------------------------------------------------\n",
    "    if 'global_orient' in params:\n",
    "        global_orient = params['global_orient'] # Shape (N, 3) - Axis Angle\n",
    "        \n",
    "        # Convert current axis-angle to Rotation object\n",
    "        rot_original = R.from_rotvec(global_orient)\n",
    "        \n",
    "        # Create a 180-degree rotation around Y-axis\n",
    "        rot_180_y = R.from_euler('y', 180, degrees=True)\n",
    "        \n",
    "        # Apply the rotation: New = Rot180 * Old\n",
    "        # (We left-multiply to apply the rotation to the global frame)\n",
    "        rot_new = rot_180_y * rot_original\n",
    "        \n",
    "        # Convert back to axis-angle and ensure correct datatype\n",
    "        params['global_orient'] = rot_new.as_rotvec().astype(global_orient.dtype)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Save Output\n",
    "    # ---------------------------------------------------------\n",
    "    base_path, ext = os.path.splitext(smplx_path)\n",
    "    \n",
    "    if ext == '.npz':\n",
    "        np.savez(smplx_path, **params)\n",
    "    else:\n",
    "        np.save(smplx_path, params)\n",
    "        \n",
    "    print(f\" -> Modified and saved to: {smplx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad015d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SMPL-X params from: mydataset/gait_753/20250617_c1_a4_Take2/split_subjects/0/fit-smplx/smplx-params.npz\n",
      " -> Modified and saved to: mydataset/gait_753/20250617_c1_a4_Take2/split_subjects/0/fit-smplx/smplx-params_rotated.npz\n"
     ]
    }
   ],
   "source": [
    "rotate_smplx_params_y_180(\"mydataset/gait_753/20250617_c1_a4_Take2/split_subjects/0/fit-smplx/smplx-params.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sum_flat(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Takes the sum over all non-batch dimensions.\n",
    "    \n",
    "    For a tensor of shape (B, F, D), it will sum over F and D,\n",
    "    resulting in a tensor of shape (B,).\n",
    "\n",
    "    :param tensor: A PyTorch tensor where the first dimension is the batch size.\n",
    "    :return: A tensor of shape (batch_size,) with the summed values.\n",
    "    \"\"\"\n",
    "    # The dimensions to sum over are all dimensions starting from the second one (index 1).\n",
    "    return tensor.sum(dim=tuple(range(1, tensor.dim())))\n",
    "def masked_l2(self, a, b, seqlen):\n",
    "    \"\"\"\n",
    "    Computes the masked L2 loss.\n",
    "    The loss is computed only on the frames before the sequence length.\n",
    "    \"\"\"\n",
    "    batch, frames, features = a.shape\n",
    "\n",
    "    # Create a mask for the sequences\n",
    "    # indices is a tensor of shape [1, N] -> [[0, 1, 2, ..., N-1]]\n",
    "    indices = torch.arange(frames, device=a.device).unsqueeze(0)\n",
    "\n",
    "    # mask is a boolean tensor of shape [B, N]\n",
    "    # It's True for frames within the original sequence length\n",
    "    mask = indices < seqlen.unsqueeze(1)\n",
    "    print(\"Mask shape:\", mask.shape)\n",
    "    print(mask)\n",
    "\n",
    "    # Expand mask to match the shape of a and b: [B, N, C]\n",
    "    mask_expanded = mask.unsqueeze(-1).expand_as(a)\n",
    "\n",
    "    # Compute squared error and apply the mask\n",
    "    loss = (a - b) ** 2\n",
    "    masked_loss = loss * mask_expanded\n",
    "    print(\"Masked loss shape:\", masked_loss)\n",
    "\n",
    "    # Normalize by the number of valid elements\n",
    "    # seqlen has shape [B], so we need to unsqueeze it for broadcasting\n",
    "    num_valid_elements = seqlen.unsqueeze(1) * features\n",
    "    # Avoid division by zero for sequences of length 0\n",
    "    num_valid_elements = torch.max(num_valid_elements, torch.ones_like(num_valid_elements))\n",
    "    print(\"Num valid elements shape:\", num_valid_elements.shape)\n",
    "    print(\"Num valid elements shape:\", num_valid_elements)\n",
    "    \n",
    "    # Sum the loss over frames and features, then normalize\n",
    "    return sum_flat(masked_loss) / num_valid_elements.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfffd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create standard inputs\n",
    "seq_lengths = torch.tensor([2, 3]) # Only using 2 and 3 frames out of 5\n",
    "a = torch.randn(2, 5, 3) # Batch 2, 5 Frames, 3 Features\n",
    "b = torch.randn(2, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2a2819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(seq_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f0c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: torch.Size([2, 5])\n",
      "tensor([[ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False]])\n",
      "Masked loss shape: tensor([[[1.3295e+00, 2.8029e-01, 1.3354e+00],\n",
      "         [7.5189e-02, 1.2598e+00, 5.7090e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.4071e-04, 5.9394e+00, 7.1982e+00],\n",
      "         [1.2933e+00, 1.2291e-01, 6.8445e-01],\n",
      "         [4.1868e-02, 5.1234e-01, 3.3537e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
      "Num valid elements shape: torch.Size([2, 1])\n",
      "Num valid elements shape: tensor([[6],\n",
      "        [9]])\n",
      "tensor([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.6649, 1.7585])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_l2(None, a, b, seq_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitviewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
